return(as.numeric(as.character(doi_count/ref_count)))
}
Get_sci_score2(test_content[9])
article_most_recent_table_sel=dplyr::filter(article_most_recent_table,art %in% art_sci_of_int)
article_most_recent_table_sel$sci_score=as.numeric(as.character(unlist(sapply(article_most_recent_table_sel$`*`,Get_sci_score2))))
article_most_recent_table_sel$sci_score[which(article_most_recent_table_sel$sci_score>1)]=1
dfcr=read.csv("dfcr_annot3_doi_filtered.csv",sep=";")
unique(article_most_recent_table_sel)%>%dplyr::select(art,sci_score)%>%
dplyr::left_join(dfcr,by=c("art"))%>%
dplyr::arrange(sci_score) %>%
#dplyr::top_n(30,wt=sci_score)%>%
ggplot(aes(reorder(art,-sci_score),sci_score,fill=type))+
geom_bar(stat="identity")+theme_classic()+scale_y_continuous(expand=c(0,0))+#coord_flip()+
scale_fill_brewer("type", palette="Dark2")+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+ggtitle("Wikipedia articles sorted by scientific score")
Get_sci_score2=function(art_text){
#  maybe doi vs ref count!
#art_text=test_content[9]
ref_regexp='<ref.*?</ref>' # in-text refs!
doi_regexp= "10\\.\\d{4,9}/[-._;()/:a-z0-9A-Z]+"
ref_fetched=str_match_all(art_text, ref_regexp)
ref_count=length(as.character(unlist(ref_fetched)))
doi_fetched=str_match_all(art_text, doi_regexp)
doi_count=length(as.character(unlist(doi_fetched)))
return(as.numeric(as.character(doi_count/ref_count)))
}
Get_sci_score2(test_content[9])
article_most_recent_table_sel=dplyr::filter(article_most_recent_table,art %in% art_sci_of_int)
article_most_recent_table_sel$sci_score=as.numeric(as.character(unlist(sapply(article_most_recent_table_sel$`*`,Get_sci_score2))))
article_most_recent_table_sel$sci_score[which(article_most_recent_table_sel$sci_score>1)]=1
dfcr=read.csv("dfcr_annot3_doi_filtered.csv",sep=";")
unique(article_most_recent_table_sel)%>%dplyr::select(art,sci_score)%>%
dplyr::left_join(dfcr,by=c("art"))%>%
dplyr::arrange(sci_score) %>%
#dplyr::top_n(30,wt=sci_score)%>%
ggplot(aes(reorder(art,-sci_score),sci_score,fill=type))+
geom_bar(stat="identity")+theme_classic()+scale_y_continuous(expand=c(0,0))+#coord_flip()+
scale_fill_brewer("type", palette="Dark2")+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+ggtitle("Wikipedia articles sorted by scientific score")
unique(article_most_recent_table_sel)%>%dplyr::select(art,sci_score)%>%
dplyr::left_join(dfcr,by=c("art"))%>%dplyr::filter(!is.na(type),!is.na(sci_score))%>%
#dplyr::arrange(sci_score) %>%
#dplyr::top_n(30,wt=sci_score)%>%
ggplot(aes(fct_reorder(type,sci_score, .fun=median,.desc=T),sci_score,fill=type))+
geom_boxplot()+theme_classic()+scale_y_continuous(expand=c(0,0))+#coord_flip()+
scale_fill_brewer("type", palette="Dark2")+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+ggtitle("Wikipedia articles sorted by scientific score")
library(europepmc)
epmc_details("10.1016/S0140-6736(19)33221-0")
epmc_search("10.1016/S0140-6736(19)33221-0")
epmc_details(doi="10.1016/S0140-6736(19)33221-0")
?epmc_details
epmc_details(ext_id = "PMC4747116", data_src = "pmc")
#' Get Article History table
#'
#' This function get an english wikipedia article title as input
#' and retrieve a table containing ids, timestamp, comment,
#' user, userid,size,content for every revisions of the given wikipedia article
#'
#'
#' @param article_name Path to the input file
#' @return A table with all revisions of the wikipedia article
#' @export
#'
#' @examples
#' get_article_full_history_table("Zeitgeber")
get_article_full_history_table=function(article_name){
what="ids|timestamp|comment|user|userid|size|content" #|parsedcomment|tags|flags
article_name_c=gsub(" ","%20",article_name)
output_table=c()
cmd=paste("https://en.wikipedia.org/w/api.php?action=query&titles=",article_name_c,"&prop=revisions&rvprop=",what,"&rvstart=01012001&rvdir=newer&format=json&rvlimit=max",sep="")
resp=GET(cmd)
parsed <- jsonlite::fromJSON(httr::content(resp, "text"), simplifyVector = T)
tt=paste("parsed$query$pages$'",names(parsed$query$pages)[1],"'$revisions",sep="")
name_tab=rep(article_name,dim(eval(parse(text=tt)))[1])
tmp_tab=eval(parse(text=tt))
output_table=cbind(art=name_tab,tmp_tab[,c("revid","parentid","user","userid","timestamp","size","comment","*")])
while(length(parsed$continue$rvcontinue)==1){
output_table_load=c()
print(parsed$continue$rvcontinue)
rvc=parsed$continue$rvcontinue
cmd=paste("https://en.wikipedia.org/w/api.php?action=query&titles=",article_name_c,"&prop=revisions&rvprop=",what,"&rvstart=01012001&rvdir=newer&format=json&rvlimit=max&rvcontinue=",rvc,sep="")
resp=GET(cmd)
parsed <- jsonlite::fromJSON(httr::content(resp, "text"), simplifyVector = T)
tt2=paste("parsed$query$pages$'",names(parsed$query$pages)[1],"'$revisions",sep="")
name_tab2=rep(article_name,dim(eval(parse(text=tt2)))[1])
tmp_tab2=eval(parse(text=tt2))
if(length(name_tab2)<1) break
output_table_load=cbind(art=name_tab2,tmp_tab2[,c("revid","parentid","user","userid","timestamp","size","comment","*")])
output_table=try(rbind(output_table,output_table_load),silent=T)
}
return(output_table)
}
#' Get Article Initial Table
#'
#' This function get an english wikipedia article title as input
#' and retrieve a table containing ids, timestamp, comment,
#' user, userid,size,content of the first revision of the given wikipedia article
#'
#'
#' @param article_name Path to the input file
#' @return A table with the initial revision of the wikipedia article
#' @export
#'
#' @examples
#' get_article_initial_table("Zeitgeber")
get_article_initial_table=function(article_name){
what="ids|timestamp|comment|user|userid|size|content" #|parsedcomment|tags|flags
article_name_c=gsub(" ","%20",article_name)
output_table=c()
cmd=paste("https://en.wikipedia.org/w/api.php?action=query&titles=",article_name_c,"&prop=revisions&rvprop=",what,"&rvstart=01012001&rvdir=newer&format=json&rvlimit=1",sep="")
resp=GET(cmd)
parsed <- jsonlite::fromJSON(httr::content(resp, "text"), simplifyVector = T)
tt=paste("parsed$query$pages$'",names(parsed$query$pages)[1],"'$revisions",sep="")
name_tab=rep(article_name,dim(eval(parse(text=tt)))[1])
tmp_tab=eval(parse(text=tt))
output_table=cbind(art=name_tab,tmp_tab[,c("revid","parentid","user","userid","timestamp","size","comment","*")])
return(output_table)
}
#' Get Article Informations Table
#'
#' This function get an english wikipedia article title as input
#' and retrieve a table containing pageids, title and length
#' of the given wikipedia article
#'
#'
#' @param article_name Path to the input file
#' @return A table with the initial revision of the wikipedia article
#' @export
#'
#' @examples
#' get_article_info_table("Zeitgeber")
get_article_info_table=function(article_name){
#article_name="Zeitgeber"
what="pageid|title|length" #|parsedcomment|tags|flags
#api.php?action=query&titles=Albert%20Einstein&prop=info&inprop=url|talkid
article_name_c=gsub(" ","%20",article_name)
output_table=c()
cmd=paste("https://en.wikipedia.org/w/api.php?action=query&titles=",article_name_c,"&prop=info&inprop=",what,"&format=json",sep="")
resp=GET(cmd)
parsed <- jsonlite::fromJSON(httr::content(resp, "text"), simplifyVector = T)
tt=paste("parsed$query$pages$'",names(parsed$query$pages)[1],"'",sep="")
#name_tab=rep(article_name,dim(eval(parse(text=tt)))[1])
tmp_tab=unlist(eval(parse(text=tt)))
#output_table=cbind(art=name_tab,tmp_tab[,c("revid","parentid","user","userid","timestamp","size","comment","*")])
return(tmp_tab)
}
#' Get Article Most Recent Table
#'
#' This function get an english wikipedia article title as input
#' and retrieve a table containing ids, timestamp, comment,
#' user, userid,size,content of the last revision of the given wikipedia article
#'
#'
#' @param article_name Path to the input file
#' @return A table with the last revision of the wikipedia article
#' @export
#'
#' @examples
#' get_article_initial_table("Zeitgeber")
get_article_most_recent_table=function(article_name){
what="ids|timestamp|comment|user|userid|size|content" #|parsedcomment|tags|flags
article_name_c=gsub(" ","%20",article_name)
output_table=c()
cmd=paste("https://en.wikipedia.org/w/api.php?action=query&titles=",article_name_c,"&prop=revisions&rvprop=",what,"&rvend=05072019&rvdir=older&format=json&rvlimit=1",sep="")
resp=GET(cmd)
parsed <- jsonlite::fromJSON(httr::content(resp, "text"), simplifyVector = T)
tt=paste("parsed$query$pages$'",names(parsed$query$pages)[1],"'$revisions",sep="")
name_tab=rep(article_name,dim(eval(parse(text=tt)))[1])
tmp_tab=eval(parse(text=tt))
output_table=cbind(art=name_tab,tmp_tab[,c("revid","parentid","user","userid","timestamp","size","comment","*")])
return(output_table)
}
#' Write Article Table to an xlsx
#'
#' This function get a wikipedia article table as input and the name of the target xls file
#'
#'
#' @param wiki_hist wiki history table
#' @param file_name output file name prefix
#' @return A table with the last revision of the wikipedia article
#' @export
#'
#' @examples
#'
#' tmpwikitable=get_article_initial_table("Zeitgeber")
#' write_wiki_history_to_xlsx(tmpwikitable,"Zeitgeber")
write_wiki_history_to_xlsx=function(wiki_hist,file_name){
wiki_hist[is.na(wiki_hist)]="-"
wiki_hist[is.null(wiki_hist)]="-"
df <- data.frame(art=wiki_hist$art,revid=wiki_hist$revid,parentid=wiki_hist$parentid,user=wiki_hist$user,userid=wiki_hist$userid,
timestamp=wiki_hist$timestamp,size=wiki_hist$size,comment=wiki_hist$comment,content=wiki_hist$`*`,stringsAsFactors=FALSE)
write.xlsx(df,paste(file_name,"wiki_table.xlsx",sep="_"), sheetName="Sheet1",  col.names=TRUE, row.names=F, append=FALSE, showNA=T)
}
#' Get Category Articles history
#'
#' This function get a list of wikipedia article titles as input
#' and create a wipipedia history table
#'
#' @param list_art list of wikipedia articles
#' @return A table with the all revisions of each wikipedia articles in the input
#' @export
#'
#' @examples
#'
#' Category_articles_history=get_category_articles_history(c("Zeitgeber","Advanced sleep phase disorder","Sleep deprivation"))
#'
get_category_articles_history=function(list_art){
dfn_art=c()
for(art in 1:length(list_art)){
dfn_load=c()
print(list_art[art])
dfn_load=try(get_article_full_history_table(list_art[art]))
#dfn_load$user=rep(user_of_int[art],dim(dfn_load)[1])
if(length(dfn_load)>1){
dfn_art=rbind(dfn_art,dfn_load)
}
}
return(dfn_art)
}
#' Get Category Articles creation
#'
#' This function get a list of wikipedia article titles as input
#' and create a wipipedia creation dates table
#'
#' @param list_art list of wikipedia articles
#' @return A table with the creations instance of each wikipedia articles in the input
#' @export
#'
#' @examples
#'
#' get_category_articles_history=get_category_articles_creation(c("Zeitgeber","Advanced sleep phase disorder","Sleep deprivation"))
#'
get_category_articles_creation=function(list_art){
dfn_art=c()
for(art in 1:length(list_art)){
dfn_load=c()
print(list_art[art])
dfn_load=try(get_article_initial_table(list_art[art]))
#dfn_load$user=rep(user_of_int[art],dim(dfn_load)[1])
if(length(dfn_load)>1){
dfn_art=rbind(dfn_art,dfn_load)
}
}
return(dfn_art)
}
#' Get Pages Names in Category
#'
#' This function get a list of wikipedia article titles as input
#' and create a wipipedia creation dates table
#'
#' @param category names of wikipedia category
#' @return list of wikipedia pages in the input category
#' @export
#'
#' @examples
#'
#' get_pagename_in_cat("Circadian rhythm")
#' # For multiple Categories
#' unique(unlist(sapply(category_list,get_pagename_in_cat)))
#'
get_pagename_in_cat=function(category){
cats2=pages_in_category("en", "wikipedia", categories =category,limit = 3000) # "Circadian rhythm"
art_of_int=c()
for(i in 1:length(cats2$query$categorymembers)){
if(length(grep("User",cats2$query$categorymembers[[i]]$title))>0){  next}
else if(length(grep("Category",cats2$query$categorymembers[[i]]$title))>0){next}
else{
#print(cats2$query$categorymembers[[i]]$title)
art_of_int=c(art_of_int,cats2$query$categorymembers[[i]]$title)
}
}
return(unlist(art_of_int))
}
doi_regexp= "10\\.\\d{4,9}/[-._;()/:a-z0-9A-Z]+" #Good enough
isbn_regexp='(?<=(isbn|ISBN)\\s?[=:]?\\s?)\\d{1,5}-\\d{1,7}-\\d{1,5}-[\\dX]' # good enough
url_regexp = "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
tweet_regexp='\\{\\{cite tweet.*?\\}\\}'
news_regexp='\\{\\{cite news.*?\\}\\}'
journal_regexp='\\{\\{cite journal.*?\\}\\}'
web_regexp='\\{\\{cite web.*?\\}\\}'
article_regexp='\\{\\{cite article.*?\\}\\}'
report_regexp='\\{\\{cite report.*?\\}\\}'
court_regexp='\\{\\{cite court.*?\\}\\}'
press_release_regexp='\\{\\{cite press release.*?\\}\\}'
book_regexp='\\{\\{cite book .*?\\}\\}'
pmid_regexp="(?<=(pmid|PMID)\\s?[=:]\\s?)\\d{5,9}"
ref_regexp='<ref>\\{\\{[c|C]ite.*?\\}\\}</ref>' # in-text refs!
cite_regexp='\\{\\{[c|C]ite.*?\\}\\}'
#doi
get_regex_citations_in_wiki_table=function(article_wiki_table,citation_regexp){
citation_fetched=str_match_all(article_wiki_table$`*`, citation_regexp)
df_citation=data.frame(revid=rep(article_wiki_table$revid,unlist(lapply(dois_fetched,length))),doi=unlist(dois_fetched))
df_citation_revid_art=dplyr::select(article_wiki_table,art,revid)%>%dplyr::right_join(df_doi,by="revid")
return(df_citation_revid_art)
}
#
#
#
# #dois_fetched_1=unlist(str_match_all(art_test$`*`[335], doi_regexp))
# annotate_doi_list_europmc=function(doi_list){
#   annotated_doi_df=c()
#   for(i in 1:length(doi_list)){ #
#     print(i)
#     print(doi_list[i])
#     annotated_dois_df_load=tryCatch(epmc_search(paste("DOI:",doi_list[i],sep="")),error = function(e) NULL)
#     if(is.null(annotated_dois_df_load)){annotated_dois_df_load=tryCatch(epmc_search(doi_list[i]),error = function(e) NULL)}
#     if(is.null(annotated_dois_df_load)){next}
#     if(dim(annotated_dois_df_load)[1]==1){
#
#       dft=melt(as.list(annotated_dois_df_load))
#       dft=cbind(dft,L2=rep(doi_list[i],dim(dft)[1]))
#       annotated_doi_df=rbind(annotated_doi_df,dft)
#     }
#     #Sys.sleep(time)
#   }
#   return(annotated_doi_df_clean=dcast(annotated_doi_df, L2 ~ L1))
#
# }
#
# library(rcrossref)
#
# annotate_doi_list_cross_ref=function(doi_list){
#   doi_bib=cr_cn(dois = doi_list,"bibentry",.progress = "text")
#
#   doi_bib_df=dcast(melt(doi_bib[-(which(lapply(doi_bib,length)==0))]), L1 ~ L2)
#
#   citation_countdf=cr_citation_count(doi = doi_bib_df$doi)
#
#   doi_bib_df=doi_bib_df%>%dplyr::left_join(citation_countdf,by=c("doi"))
#
#   return(doi_bib_df)
# }
#
#
#
# parse_cite_type=function(citation){
#   get_cite=gsub("\\{\\{[c|C]ite","",as.character(citation))
#   get_cite=gsub("\\{\\{[c|C]ite","",get_cite)
#   get_cite_type=unlist(strsplit(get_cite,"\\|"))[1]
#   get_cite_type=gsub("\\s", "", get_cite_type)
#   get_cite_type=tolower(get_cite_type)
#   return(get_cite_type)
# }
#
# extract_citations=function(art_text){
#   cite_regexp='\\{\\{[c|C]ite.*?\\}\\}'
#
#   #cite
#   cite_fetched=str_match_all(art_text, cite_regexp)
#
#   cite=as.character(unlist(cite_fetched))
#
#   return(cite)#df_cite_revid_art=dplyr::select(article_most_recent_table,art,revid)%>%dplyr::right_join(df_cite,by="revid")
#
# }
#
# extract_wikihypelinks=function(art_text){
#   wiki_regexp='\\[\\[.*?\\]\\]'
#
#   #cite
#   wikihypelinks_fetched=str_match_all(art_text, wiki_regexp)
#
#   wikihypelinks=as.character(unlist(wikihypelinks_fetched))
#
#   return(wikihypelinks)#df_cite_revid_art=dplyr::select(article_most_recent_table,art,revid)%>%dplyr::right_join(df_cite,by="revid")
#
# }
#
#
#
# replace_wikihypelinks=function(art_text){
#   whl=extract_wikihypelinks(art_text)
#   whl_cleaned=gsub("\\[\\[","",whl)
#   whl_cleaned=gsub("\\]\\]","",whl_cleaned)
#
#   whl_cleaned=sapply(whl_cleaned,function(x) as.character(unlist(strsplit(x,"\\|")))[1])
#
#   art_text=mgsub(art_text,whl,whl_cleaned)
#   return(art_text)
#
# }
#
# parse_article_ALL_citations=function(art_text){
#   #art_text=test_content[9]  # OK
#   get_cite=as.character(sapply(extract_citations(art_text),replace_wikihypelinks))
#   cite_types=sapply(get_cite,parse_cite_type)
#   get_cite=gsub("\\{\\{[c|C]ite","",as.character(get_cite))
#   get_cite=gsub("\\{\\{[c|C]ite","",get_cite)
#   get_cite=gsub("\\{\\{","",get_cite)
#   get_cite=gsub("\\}\\}","",get_cite)
#
#   get_cite_subfield=sapply(get_cite, function(x) unlist(strsplit(x,"\\|"))[2:length(unlist(strsplit(x,"\\|")))])
#
#   df_out=data.frame(type=rep(cite_types,lapply(get_cite_subfield,length)),id_cite=rep(1:length(get_cite),lapply(get_cite_subfield,length)),
#                     colsplit(string=unlist(get_cite_subfield), pattern="=", names=c("variable", "value")))
#
#   df_out$variable=gsub(" ","",df_out$variable)
#
#   #dcast(df_out,id_cite~Part1,value.var="Part2")
#   return(df_out)
# }
#
# Get_sci_score=function(art_text){ #  maybe doi vs ref count!
#   extracted_cite=tryCatch(extract_citations(art_text),error = function(e) 0)
#   cite_type=sapply(extracted_cite,parse_cite_type)
#   all_cite_sum= tryCatch(sum(table(cite_type)),error = function(e) 0)
#   journal_cite= tryCatch(table(cite_type)[which(names(table(cite_type))=="journal")],error = function(e) NA)
#   if(length(journal_cite)==0){return(0)}
#   return(as.numeric(as.character(journal_cite/all_cite_sum)))
# }
#
# get_refCount=function(art_text){
#   ref_regexp='<ref.*?</ref>'
#   ref_fetched=str_match_all(art_text, ref_regexp)
#   ref_count=length(as.character(unlist(ref_fetched)))
#   return(as.numeric(as.character(ref_count)))
# }
#
# get_doi_count=function(art_text){
#   doi_regexp= "10\\.\\d{4,9}/[-._;()/:a-z0-9A-Z]+"
#   doi_fetched=str_match_all(art_text, doi_regexp)
#   doi_count=length(as.character(unlist(doi_fetched)))
#   return(as.numeric(as.character(doi_count)))
# }
#
# get_ISBN_count=function(art_text){
#   ISBN_regexp='(?<=(isbn|ISBN)\\s?[=:]?\\s?)\\d{1,5}-\\d{1,7}-\\d{1,5}-[\\dX]'
#   ISBN_fetched=str_match_all(art_text, ISBN_regexp)
#   ISBN_count=length(as.character(unlist(ISBN_fetched)))
#   return(as.numeric(as.character(ISBN_count)))
# }
#
#
# Get_sci_score2=function(art_text){ # Better !
#   #  maybe doi vs ref count!
#   #art_text=test_content[9]
#   ref_regexp='<ref.*?</ref>' # in-text refs!
#   doi_regexp= "10\\.\\d{4,9}/[-._;()/:a-z0-9A-Z]+"
#
#   ref_fetched=str_match_all(art_text, ref_regexp)
#   ref_count=length(as.character(unlist(ref_fetched)))
#
#   doi_fetched=str_match_all(art_text, doi_regexp)
#   doi_count=length(as.character(unlist(doi_fetched)))
#
#   return(as.numeric(as.character(doi_count/ref_count)))
# }
#
# Get_source_type_counts=function(art_text){ # Good!
#   extracted_cite=tryCatch(extract_citations(art_text),error = function(e) 0)
#   cite_type=sapply(extracted_cite,parse_cite_type)
#
#   cite_source_count= tryCatch(table(cite_type),error = function(e) NA)
#   if(length( cite_source_count)==0){return(NA)}
#   return(as.data.frame(cite_source_count))
#
# }
#
load("C:/Users/jsobel/Desktop/Wiki_covid-19/ws.RData")
get_regex_citations_in_wiki_table(article_most_recent_table_sel,doi_regexp)
library(XML)
library(dplyr)
library(reshape)
library(textreuse)
library("xlsx")
library("lubridate")
library(httr)
if (Sys.getenv("JAVA_HOME")!="")
Sys.setenv(JAVA_HOME="")
library(rJava)
library(WikipediR)
#library(xml2)
library(jsonlite)
library(curl)
library(lubridate) # for working with dates
library(ggplot2)  # for creating graphs
library(scales)   # to access breaks/formatting functions
library(gridExtra) # for arranging plots
library("rcrossref")
library(timevis)
library(Rismed)
library(ggridges)
library("reshape2")
library(stringr)
get_regex_citations_in_wiki_table(article_most_recent_table_sel,doi_regexp)
doi_regexp
head(article_most_recent_table_sel)
article_most_recent_table_sel=dplyr::filter(article_most_recent_table,art %in% art_sci_of_int)
article_most_recent_table_sel$sci_score=as.numeric(as.character(unlist(sapply(article_most_recent_table_sel$`*`,Get_sci_score2))))
article_most_recent_table_sel$sci_score[which(article_most_recent_table_sel$sci_score>1)]=1
get_regex_citations_in_wiki_table(article_most_recent_table_sel,doi_regexp)
head(article_most_recent_table_sel)
head(article_most_recent_table_sel$`*`)[1]
get_regex_citations_in_wiki_table=function(article_wiki_table,citation_regexp){
citation_fetched=str_match_all(article_wiki_table$`*`, citation_regexp)
df_citation=data.frame(revid=rep(article_wiki_table$revid,unlist(lapply(citation_fetched,length))),citation_fetched=unlist(citation_fetched))
df_citation_revid_art=dplyr::select(article_wiki_table,art,revid)%>%dplyr::right_join(df_citation,by="revid")
return(df_citation_revid_art)
}
get_regex_citations_in_wiki_table(article_most_recent_table_sel,doi_regexp)
setwd("C:/Users/jsobel/Desktop/Wiki_covid-19/WikiCitationHistoRy")
setwd("C:/Users/jsobel/Desktop/Wiki_covid-19/WikiCitationHistoRy")
roxygenise()
library("roxygen2")
roxygenise()
vignette("roxygen2")
devtools::document()
devtools::document()
roxygenise()
install("WikiCitationHistoRy")
setwd("C:/Users/jsobel/Desktop/Wiki_covid-19/WikiCitationHistoRy")
library("devtools")
install("WikiCitationHistoRy")
install("WikiCitationHistoRy.r")
